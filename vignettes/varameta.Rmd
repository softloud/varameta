---
title: "varameta::"
output: 
  rmarkdown::html_vignette:
    number_sections: true
bibliography: references.bib
vignette: >
  %\VignetteIndexEntry{varmeta}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, message=FALSE}

# for reproducibility
set.seed(39)

# packages
library(varameta)
library(tidyverse)
library(simeta)
library(panda)

# move to parameter when done with chunk-wise?
sample_size <- 10

```

# Objective of the `varameta::` package

The `varameta::` package aims to bridge the toolchain gap from a dataset containing medians to a software package, such as `metafor::` [@viechtbauerConductingMetaanalysesMetafor2010], for meta-analysis.


```{r, fig.align='center', fig.width=6}
panda("varameta:: is for meta-analysing medians")

```



# A minimal demonstration of calculating the variance of the sample median

```{r, fig.align='center', fig.width=6}
panda("show me the code!")
```


## Compute standard error of the sample median

```{r}
# get a sample
a_sample <- rexp(sample_size)

# get standard error of the sample median
effect_se(
  centre = median(a_sample),
  spread = IQR(a_sample),
  n = length(a_sample),
  centre_type = "median",
  spread_type = "iqr"
)

```


## Vectorised calculations for dataframes

Here we borrow a function from the companion `simeta::` package. See below for details.

```{r}
# generate random meta-analysis dataset
# one row per study
(ma_sample <- sim_stats() %>%
  # filter down to one group per study
  dplyr::filter(group == "control"))


ma_sample %>%
  # append a column with the standard error of the median for each study
  mutate(effect_se = pmap_dbl(
    list(centre = effect,
         spread = effect_spread,
         n = n),
    effect_se,
    centre_type = "median",
    spread_type = "iqr"
  ))


```


# Calculate the standard error of mean or median based on effect, spread, and sample size

A wrapper function `effect_se` provides a quick method of calculating the error of an effect based on its measure of effect, spread, sample size. 

Consider a randomly generated sample.

```{r sample}
(a_sample <- rlnorm(sample_size,-1, 0.1))

# summary statistics for this sample
a_sample %>% log() %>% summary()
```

Taken from a lognormal distribution $\mathrm{lognormal}(-1, 0.1^2)$. 

```{r plot, echo=FALSE, fig.align='center', fig.width=6}

# taken from lognormal distribution
tibble(x = c(0, 1)) %>%
  ggplot(aes(x = x)) +
  geom_density(
    data = tibble(x = a_sample),
    aes(x = x),
    alpha = 0.5,
    fill = "darkgrey",
    colour = "darkgrey"
  ) +
  stat_function(
    fun = dlnorm,
    args = list(meanlog = -1,
                sdlog = 0.1),
    linetype = "dotted"
  ) + 
  labs(title = stringr::str_wrap("Sample from log-normal density", 30),
       subtitle = stringr::str_wrap("Sample density in grey fill; true density, black dotted line"),
       caption = "lognormal(-1, 0.01)")



```

```{r effect se median}
# calculate the standard error based on the median, interquartile range, and
# sample size

effect_se(
  centre = median(a_sample),
  spread = IQR(a_sample),
  n = length(a_sample),
  centre_type = "median",
  spread_type = "iqr"
)


```


## standard error of the sample median

## standard error of the sample mean

```{r}
# take a sample
se_mean_eg_sample <- rexp(sample_size)

```

Now, we wish to calculate the standard error $s/\sqrt n$ of the sample mean, calcualted with the the sample standard deviation and the squared-root of the sample size.


```{r}

# mean and sd
effect_se(
  centre = mean(se_mean_eg_sample),
  spread = sd(se_mean_eg_sample),
  n = length(se_mean_eg_sample),
  centre_type = "mean",
  spread_type = "sd"
)

# compare
sd(se_mean_eg_sample) /
  sqrt(length(se_mean_eg_sample))

# mean and var
effect_se(
  centre = mean(se_mean_eg_sample),
  spread = var(se_mean_eg_sample),
  n = length(se_mean_eg_sample),
  centre_type = "mean",
  spread_type = "var"
)

# compare
sqrt(var(se_mean_eg_sample) /
  length(se_mean_eg_sample))


```

# Vectorised calculations for meta-analysis datasets

```{r, fig.align='center', fig.width=6}
panda("vectorisation is my favourite part of R", panda = 14)
```


```{r}
# borrowing from sister package simeta:: to simulate a dataset
(meta_data <- sim_stats() %>% 
  dplyr::filter(group == "control"))

# add a vecorised
# todo function this (after report)
meta_data %>% 
  mutate(
    effect_se = pmap_dbl(
      list(centre = effect, spread = effect_spread, n = n),
      effect_se,
      centre_type = "median",
      spread_type = "iqr"
    )
  )

```


# Other estimators

```{r, fig.align='center', fig.width=6}
panda(
  panda = 49,
  "varameta:: provides several estimators for the variances of the sample median")
```


All other estimators available for meta-analysis are provided in `varameta::` for use in a compative analysis. 

# References
